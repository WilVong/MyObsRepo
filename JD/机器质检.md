# 1.样本
未标注样本（8000个文件）：
	![[dialog8000.xlsx]]
## 1.1验收标准（暂行）：
1，验收方式：抽查
2，验收样本量：抽查30-50条
3，验收指标：
标注内容（问题/答案）是否准确
标注是否符合规范
是否有漏标
起止位置是否准确
验收标准：错误率达到10%-15%，不满足结账标准
## 1.2 成本
成本维度：
外部团队： 3.5/条*2000条= 7000元，一周完成
地图团队： 目前评估1.7条/h，2000条需要147人天，大概30人一起做1周 500/人天*147= 73500元
准确率维度：
相差不大，通过反馈试标注错误结果返回给其他团队，可以进一步提升准确率
# 2.调研
## 2.1方案
	![[Pasted image 20230905005239.png]]
## 2.2现状
历史机器质检结果对人工质检结果正确率：49条线索平均正确率44.6%，要达到机器质检替代人工质检的目标需要优化提高质检结果准确率。
问题包括问题识别错误，问题段落划分错误，答案识别错误。这些问题的主要原因包括asr录音转文本误差，样本数量过少，人工规则覆盖面有限等。

## 2.3历史机器质检case归类总结：
### ASR录音转文本错误 @产品同学
1、顾客的文本未分出来，包含在客服文本里，导致答案没分出来
2、极兔未转文本成功
- 顾客未回答，客服主观判断（22条/50条）
有客户未回答选项，这类问题可选择客户未回答，可以调研一下为什么质检员没有选择客户未回答而是主观判断填写 @产品同学 @算法同学
### 答案抽取错误 （4条）
1、顾客回答表单问题回答有的习惯带单位，有的习惯不带单位，如单量有的回答100左右吧，有的回答1000单，重量段有的习惯回答200斤，有的习惯回答200到300； 基于规则匹配加上单位漏识别，不加单位误识别，如重量段和货品价值和竞品价格都涉及到数量； 基于语义生成效果更好；
2、 几，十几，几百规则未覆盖
3、重量段和货品价值和竞品价格用规则判断容易混淆，语义识别准确率更高
### 问题抽取或段落切分错误
1、问题没有匹配到， 客户询问不规范基于规则没有匹配到，基于语义更好识别
2、两个问题之间的段落为当前问题的答案段落，当前取答案段落第一行作为段落切分结果
### 转义 （1条）
客户回答单量预估是20公斤，20公斤位于边界，如0-20，20-50选项都有20公斤，和一线或者产品侧协商这种情况要落到哪个区间
## 2.4模型调研
答案抽取模型:macbert-large
![](https://apijoyspace.jd.com/v1/files/kUaDRpMkELYPMw9OAYfM/link)
注: 整体效果：macbert >roberta >bert
使用我们自己的标注语料验证，目前自然语言生成模型选项类别验证准确率为53.4%， 自然语言生成文本答案准确率为72.5%， 均高于规则 28%（14/50）[?]

## 2.5段落切分最初逻辑
step1 遍历对话的每一行，基于正则表达式匹配固定的询问词或者表达方式，对每一行进行问询分类
问题触发pattern:
```
'order_num':['发.*多少[单件]','单量多少单','多少单量''发.*几件','发.*几单','多少单.*能寄','多少单.*能发'],
'order_value':['贵不贵','价值.*多少','[货产][品物].*价[值格]','[货产][品物].*值多少]','[货产][品物].*单价','多贵'],
'order_weight':['重不重','多重','多少公斤','几公斤','多少千克','几千克','多少斤','几斤','货[物品].*重量[是]?[吧吗]?','重量.*多少.*[公千]?[斤克]?','是.*[公千]?[斤克].*[吧吗]','.*重量.*是不是'], #去掉重量 保准确
'order_efficiency':['时效.*[要需]求','时效性.*[要需]求','时效有[要需]求','时效*有没有.*[要需]求','时效性.*有.*特殊[要需]求','时效性.*有没有.*[要需]求','时效性.*有.*[要需]求'],
'goods_type':['什么.*东西','[寄发].+是吧','发.*一些什么','哪些东西','什么[物产货商][品件]','什么行业','什么商品','大件','小件','大小件','发的是.*啥','啥物[品件]','发啥'],
'business_company':['哪个物流','什么快运','什么快递','哪个快递','其他快递','哪个公司','合作.*公司','合作的物流','合作的快递','发京东.*是[吗吧]','发顺丰.*是[吗吧]','发京东.*[多]?[吗吧]','发顺丰.*[多]?[吗吧]','顺丰.+多少','[申中圆]通.+多少','顺丰.+优惠','顺丰.+折扣','用.+京东','用.+顺丰'],
'special_requirement':['其他特殊','特殊要求','其他.*物流[要需]求','其他[要需]求','冷链.*[要需]求','冷藏.*[要需]求','仓储.*[要需]求','港澳台','额外.*要求','需要.*冷[藏链]']
}
#伪代码
for line in lines: #**遍历python IO读取的输入行**
	if re.match(question_pattern): #**如果正则匹配到pattern**
	question_line_index[question_type].append(line_index) #**问题行索引加一**
```
step2  将每一个类型问题出现的行号中取最后一个（同一个类型问题可能出现在多行），然后将所有问题的行号进行排序
```
for question_type in question_types:
	start_indexs[question_type] =question_line_index[question_type][-1] #对于每个问题类型选择问题触发的最后一行作为段落起始位置
	selected_start_indexs.append(question_line_index[question_type][-1] ) #同时将每个问题的起始位置加入一个列表
selected_start_indexs=sorted(selected_start_indexs) #排序起始位置列表
```
step3 截取每个问题出现的位置到下一个问题出现的位置之间的对话段落作为答案所在段落并返回给答案抽取模块
```
for question_type in question_types:
	s=start_index[question_type]
	paragraphs[question_type]= lines [ s : selected_start_indexs [ selected_start_indexs.index(s) +1 ] ] #获取下一个问题的其实位置并截取段落
return paragraphs #将段落返回给答案抽取模块
```

## 2.6chatgpt评估
ChatGPT文本生成，人工标注，评估效果：
![[对话质检结果_20230303_v2.xlsx]]
效果评估：（未做转义映射到选项，仅根据答案文本判断是否准确）
![](https://apijoyspace.jd.com/v1/files/C26cculR4pbDGO3a1CKL/link)
# 3.模型
## 3.1问题抽取文本分类模型
[https://github.com/ymcui/Chinese-BERT-wwm](https://github.com/ymcui/Chinese-BERT-wwm)
科大讯飞和哈工大合作预训练模型
## 3.2答案抽取自然语言生成模型 - 生成选项
macbert large
### 链接：
[https://huggingface.co/luhua/chinese_pretrain_mrc_macbert_large?context=%E5%AE%A2%E6%9C%8D%3A%E5%93%A6%EF%BC%8C%E5%A5%BD%E7%9A%84%EF%BC%8C%E6%88%91%E4%BA%86%E8%A7%A3%EF%BC%8C%E9%82%A3%E6%82%A8%E8%BF%99%E8%BE%B9%E4%B8%80%E4%B8%AA%E6%9C%88%E5%A4%A7%E6%A6%82%E4%BC%9A%E5%8F%91%E5%A4%9A%E5%B0%91%E4%BB%B6%E5%91%A2%EF%BC%9F%E9%A1%BE%E5%AE%A2%3A%E4%BB%96%E8%BF%99%E4%B8%AA%E5%BE%88%E9%9A%BE%E5%8F%97%EF%BC%8C%E5%B0%B1%E7%9C%8B%E4%B8%9A%E5%8A%A1%E7%9C%8B%E8%BF%99%E4%B8%AA%E4%B8%9A%E5%8A%A1%E3%80%82%E5%AE%A2%E6%9C%8D%3A%E4%B8%80%E4%B8%AA%E6%9C%88%E6%9C%89%E6%B2%A1%E6%9C%89%E4%B8%80%E5%8D%83%E5%A4%9A%E4%BB%B6%E5%B7%AE%E4%B8%8D%E5%A4%9A%E5%90%A7%EF%BC%9F%E9%A1%BE%E5%AE%A2%3A%E6%B2%A1%E6%9C%89&question=%E9%A1%BE%E5%AE%A2%E4%B8%80%E4%B8%AA%E6%9C%88%E5%8F%91%E5%A4%9A%E5%B0%91%E4%BB%B6%EF%BC%9F](https://huggingface.co/luhua/chinese_pretrain_mrc_macbert_large?context=%E5%AE%A2%E6%9C%8D%3A%E5%93%A6%EF%BC%8C%E5%A5%BD%E7%9A%84%EF%BC%8C%E6%88%91%E4%BA%86%E8%A7%A3%EF%BC%8C%E9%82%A3%E6%82%A8%E8%BF%99%E8%BE%B9%E4%B8%80%E4%B8%AA%E6%9C%88%E5%A4%A7%E6%A6%82%E4%BC%9A%E5%8F%91%E5%A4%9A%E5%B0%91%E4%BB%B6%E5%91%A2%EF%BC%9F%E9%A1%BE%E5%AE%A2%3A%E4%BB%96%E8%BF%99%E4%B8%AA%E5%BE%88%E9%9A%BE%E5%8F%97%EF%BC%8C%E5%B0%B1%E7%9C%8B%E4%B8%9A%E5%8A%A1%E7%9C%8B%E8%BF%99%E4%B8%AA%E4%B8%9A%E5%8A%A1%E3%80%82%E5%AE%A2%E6%9C%8D%3A%E4%B8%80%E4%B8%AA%E6%9C%88%E6%9C%89%E6%B2%A1%E6%9C%89%E4%B8%80%E5%8D%83%E5%A4%9A%E4%BB%B6%E5%B7%AE%E4%B8%8D%E5%A4%9A%E5%90%A7%EF%BC%9F%E9%A1%BE%E5%AE%A2%3A%E6%B2%A1%E6%9C%89&question=%E9%A1%BE%E5%AE%A2%E4%B8%80%E4%B8%AA%E6%9C%88%E5%8F%91%E5%A4%9A%E5%B0%91%E4%BB%B6%EF%BC%9F)
## 3.3 答案抽取自然语言生成模型 - 生成选项或原文片段
unilm
[https://github.com/YunwenTechnology/Unilm](https://github.com/YunwenTechnology/Unilm)
NER
https://github.com/z814081807/DeepNER （未采用）
## 3.4 调研小样本学习答案抽取任务
[https://huggingface.co/PaddlePaddle/utc-large?candidateLabels=%E5%8D%95%E9%87%8F%2C%E8%B4%A7%E5%93%81%E4%BB%B7%E6%A0%BC%2C%E9%87%8D%E9%87%8F%2C%E7%89%B9%E6%AE%8A%E9%9C%80%E6%B1%82%2C%E6%97%B6%E6%95%88%E8%A6%81%E6%B1%82%2C%E8%BF%90%E5%8D%95%E4%BB%B7%E6%A0%BC&multiClass=true&text=%E6%82%A8%E6%9C%89%E4%BB%93%E5%82%A8%E6%88%96%E8%80%85%E5%86%B7%E9%93%BE%E9%9C%80%E6%B1%82%E5%90%97%EF%BC%9F](https://huggingface.co/PaddlePaddle/utc-large?candidateLabels=%E5%8D%95%E9%87%8F%2C%E8%B4%A7%E5%93%81%E4%BB%B7%E6%A0%BC%2C%E9%87%8D%E9%87%8F%2C%E7%89%B9%E6%AE%8A%E9%9C%80%E6%B1%82%2C%E6%97%B6%E6%95%88%E8%A6%81%E6%B1%82%2C%E8%BF%90%E5%8D%95%E4%BB%B7%E6%A0%BC&multiClass=true&text=%E6%82%A8%E6%9C%89%E4%BB%93%E5%82%A8%E6%88%96%E8%80%85%E5%86%B7%E9%93%BE%E9%9C%80%E6%B1%82%E5%90%97%EF%BC%9F)

## 3.5 问题分类模型代码
![[question_classificaiton_bert.ipynb]]
### 超参数
```#参数
MAX_LEN = 120
TRAIN_BATCH_SIZE = 32
VALID_BATCH_SIZE = 32
EPOCHS = 1
LEARNING_RATE = 2e-05
tokenizer = BertTokenizer.from_pretrained('/home/wangran108/code/model_file/chinese-electra-180g-large-discriminator')
```
### 原始处理后的数据
![[Pasted image 20230914012718.png]]
### Dataset实现
Dataset类是一个抽象类，所有的数据集想要在数据与标签之间建立映射，都需要继承这个类，所有的子类都需要重写__getitem__方法，该方法根据索引值获取每一个数据并且获取其对应的Label，子类也可以重写__len__方法，返回数据集的size大小
```
from torch.utils.data import Dataset
class CustomDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_len):
        self.tokenizer = tokenizer
        self.data = dataframe
        self.comment_text = dataframe.comment_text
        self.targets = self.data.list
        self.max_len = max_len
    def __len__(self):
        return len(self.comment_text)
    def __getitem__(self, index):
        comment_text = str(self.comment_text[index])
        comment_text = " ".join(comment_text.split())
        inputs = self.tokenizer.encode_plus(
            comment_text,
            None,
            add_special_tokens=True,
            max_length=self.max_len,
            pad_to_max_length=True,
            return_token_type_ids=True
        )
        ids = inputs['input_ids']
        mask = inputs['attention_mask']
        token_type_ids = inputs["token_type_ids"]
        return {
            'ids': torch.tensor(ids, dtype=torch.long),
            'mask': torch.tensor(mask, dtype=torch.long),
            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),
            'targets': torch.tensor(self.targets[index], dtype=torch.float)
        } 
#分词器
tokenizer = BertTokenizer.from_pretrained('bert_pretrain')
sents = [ '人工智能是计算机科学的一个分支。', '它企图了解智能的实质。', '人工智能是一门极富挑战性的科学。', ]
ids = tokenizer.encode_plus(sents[0])
print(ids)
# {'input_ids': [1, 8, 35, 826, 52, 10, 159, 559, 98, 147, 18, 5, 7, 27, 59, 414, 12043, 2], 
#'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
#'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
#token_type_ids表示输入的第一部分(0)还是第二部分(1),应用于NSP任务
#attention_mask: 有时，需要将多个不同长度的sentence，统一为同一个长度，例如128 dim. 此时我们会需要加padding，以此将一些长度不足的128的sentence，用1进行填充。为了让模型avoid performing attention on padding token indices. 所以这个需要加上这个属性。如果处理的文本是一句话，就可以不用了。如果不传入attention_mask时，模型会自动全部用1填充。
```
attention_mask示例
```
input_ids = tokenizer(["I love China","I love my family and I enjoy the time with my family"], padding=True)
# print:
# {
#'input_ids': [[0, 100, 657, 436, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1],
#              [0, 100, 657, 127, 284, 8, 38, 2254, 5, 86, 19, 127, 284, 2]], 
#'attention_mask': [[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
#                   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]
#}
```
### 模型实现
```
# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. 

class BERTClass(torch.nn.Module):
    def __init__(self):
        super(BERTClass, self).__init__()
        self.l1 = transformers.ElectraModel.from_pretrained('/home/wangran108/code/model_file/chinese-electra-180g-large-discriminator')
        self.l2 = torch.nn.Dropout(0.5)
        self.l3 = torch.nn.Linear(1024, 12)
        self.dense = torch.nn.Linear(1024, 1024)
        self.activation =torch.nn.Tanh()
        # self.pooler=torch.mean()
    def forward(self, ids, mask, token_type_ids):
        # _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids,return_dict=False)
        hidden_states= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids,return_dict=True).last_hidden_state
        first_token_tensor = hidden_states[:, 0]
        pooled_output = self.dense(first_token_tensor)
        pooled_output = self.activation(pooled_output)
        # output_1=self.l1(pooled_output)
        # # output_1=torch.mean(output_1)
        output_2 = self.l2(pooled_output)
        output = self.l3(output_2)

        # output=self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids,return_dict=False)[0]
        return output
###
model = BERTClass()
model.to('cuda')
model=torch.nn.DataParallel(model )#数据并行
def loss_fn(outputs, targets):
    return torch.nn.BCEWithLogitsLoss()(outputs, targets)
optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)
```
### 模型结构(print(model))
```
BERTClass(
  (l1): ElectraModel(
    (embeddings): ElectraEmbeddings(
      (word_embeddings): Embedding(21128, 1024, padding_idx=0)
      (position_embeddings): Embedding(512, 1024)
      (token_type_embeddings): Embedding(2, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): ElectraEncoder(
      (layer): ModuleList(
        (0): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
	    ...(1-23)
      )
    )
  )
  (l2): Dropout(p=0.5, inplace=False)
  (l3): Linear(in_features=1024, out_features=11, bias=True)
  (dense): Linear(in_features=1024, out_features=1024, bias=True)
  (activation): Tanh()
)
```
### 训练
```
def train(epoch):
    model.train()
    for _,data in enumerate(tqdm(training_loader, 0)):
        ids = data['ids'].to(device, dtype = torch.long)
        mask = data['mask'].to(device, dtype = torch.long)
        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)
        targets = data['targets'].to(device, dtype = torch.float)
        outputs = model(ids, mask, token_type_ids)
        optimizer.zero_grad()
        loss = loss_fn(outputs, targets)#1.获取loss
        if _%5000==0:
            print(f'Epoch: {epoch}, Loss:  {loss.item()}')
        optimizer.zero_grad()#2.清空过往梯度
        loss.backward()#3.反向传播，计算当前梯度
        optimizer.step()#4.根据梯度更新网络参数
        #1-4简单的说就是进来一个batch的数据，计算一次梯度，更新一次网络
        torch.cuda.empty_cache()#释放显存：如果显存资源比较紧缺，可以在每个epoch开始时释放下不用的显存资源
```
torch.cuda.empty_cache：因为PyTorch是有缓存区的设置的，意思就是一个Tensor就算被释放了，进程也不会把空闲出来的显存还给GPU，而是等待下一个Tensor来填入这一片被释放的空间。所以我们用nvidia-smi/gpustat看到的显存占用不会减少，用可以该方法清空缓冲区
### 梯度累加：
1.获取loss：输入图像和标签，通过infer计算得到预测值，计算损失函数；
2.loss.backward() 反向传播，计算当前梯度；
3.多次循环步骤1-2，不清空梯度，使梯度累加在已有梯度上；
4.梯度累加了一定次数后，先optimizer.step() 根据累计的梯度更新网络参数，然后optimizer.zero_grad() 清空过往梯度，为下一波梯度累加做准备
5.总结：梯度累加则实现了batchsize的变相扩大，如果accumulation_steps（累加步数）为8，则batchsize '变相' 扩大了8倍

### 训练指标
```
for epoch in range(EPOCHS):
    outputs, targets = validation(epoch,training_loader)
    outputs = np.array(outputs) >= 0.5
    outputs=outputs.astype('int')
    targets=np.array(targets).astype('int')
    accuracy = metrics.accuracy_score(targets, outputs)
    f1_score_micro = metrics.f1_score(targets, outputs, average='micro')
    f1_score_macro = metrics.f1_score(targets, outputs, average='macro')
    print(f"Accuracy Score = {accuracy}")
    print(f"F1 Score (Micro) = {f1_score_micro}")
    print(f"F1 Score (Macro) = {f1_score_macro}")
    torch.cuda.empty_cache()
# Accuracy Score = 0.9858736059479554
# F1 Score (Micro) = 0.9944282302998142
# F1 Score (Macro) = 0.9941247820818416
```

### 预测
```
model.eval()
inputs = tokenizer.encode_plus(
        '请问发快递',
        None,
        add_special_tokens=True,
        max_length=120,
        pad_to_max_length=True,
        return_token_type_ids=True
    )
ids = torch.tensor([inputs['input_ids']], dtype=torch.long).to(device, dtype = torch.long)
mask = torch.tensor([inputs['attention_mask']]).to(device, dtype = torch.long)
token_type_ids =torch.tensor([inputs['token_type_ids']]).to(device, dtype = torch.long)

outputs = model(ids, mask, token_type_ids)
print(torch.sigmoid(outputs))#output需要经sigmoid得到概率
# tensor([[0.0265, 0.9401, 0.0306, 0.0242, 0.0287, 0.0323, 0.0392, 0.0201, 0.0293,0.0178, 0.0208, 0.0162]], device='cuda:0', grad_fn=<SigmoidBackward0>)
# np.argmax(outputs.cpu().detach().numpy().tolist(),axis=-1)[0]
print(torch.topk(torch.sigmoid(outputs),3)[1][0].cpu().numpy())
# array([1, 6, 5])
```
### 段落分割
```
def cut_dialog_doc(lines,label_num,model,tokenizer,device):
    #lines:list，顾客和客服的对话list，'客服\n','顾客\n',会占一个位置
    #label_num：11
    question_lines={}
    for i in range(1,label_num):
        question_lines[i]=[]
    row_ids_list=[]
    return_questions={}#问题类dic,key:问题index,val:问题类
    for j in range(1,(len(lines))):       
		1.遍历list，期间跳过不用预测的句子，对句子预处理：两->二
		2.使用分词器编码，送入模型得到各类别概率
		3.最大概率超过阈值0.5时，加入question_lines对应问题key的list
		4.兜底使不满足阈值也要确保有行数：如果最大概率的label不为无类别，且question_lines对应问题key没有句子索引放入，将这句话放入对应key的list
		5.topk概率的其他label概率如果大于阈值0.5，也将同时触发多问题，放入对应key的list
	#question_lines最终为：{1: [21],2: [29],3: [29],4: [41],5: [],6: [33]，7: [37],8: [17],9: [],10: []}
    6.question_lines个问题对应的value list取最后一个出现位置，最终question_lines：{1: 21, 2: 29, 3: 29, 4: 41, 5: None, 6: 33, 7: 37, 8: 17, 9: None, 10: None}
        
    row_ids_list=sorted(list(set(row_ids_list)))
    #row_ids_list：排序后各问题触发的位置，eg：[17, 21, 29, 33, 37, 41]
    
    for key in question_lines.keys():
		7.遍历question_lines，对每个问题初始化问题类对象q为空
		8.根据各问题触发位置，划分段落到问题对象q，划分依据：问题触发行到下一个问题触发
    9.case处理1：4:'合作公司'段落为空时，将5:'竞品价格'赋值
	10.case处理2：8:'货物类型'段落为空时,不做划分，将整段赋值
    return return_questions,lines
```
## 3.6 MQC代码
roberta_mrc
### 3.6.1 超参数
#### tokenizer分词器
added_tokens:
0:[PAD] 100:[UNK] 101:[CLS] 102:[SEP] 103:[MASK]
#### cconfig.json
```
- attention_probs_dropout_prob:0.1 #
- directionality:"bidi"
- hidden_act:"gelu" #
- hidden_dropout_prob:0.1 #
- hidden_size:1024 #
- initializer_range:0.02
- intermediate_size:4096
- layer_norm_eps:1e-12
- max_position_embeddings:512 #
- model_type:"bert"
- num_attention_heads:16 #
- num_hidden_layers:24 #
- pad_token_id:0
- pooler_fc_size:768 #
- pooler_num_attention_heads:12
- pooler_num_fc_layers:3
- pooler_size_per_head:128
- pooler_type:"first_token_transform"
- type_vocab_size:2
- vocab_size:21128 #
```
### 3.6.2 github细节
```
#train_bert.sh
--per_gpu_train_batch_size 4 \
--per_gpu_eval_batch_size 32 \
--learning_rate 2e-5 \
--gradient_accumulation_steps 1 \
--num_train_epochs 2.0 \
--max_seq_length 384 \
--doc_stride 128 \
--max_query_length 32 \
--max_answer_length 64 \
--n_best_size 10 \
```
### 3.6.3结构
```
#print(answer_model)
BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 1024, padding_idx=0)
      (position_embeddings): Embedding(512, 1024)
      (token_type_embeddings): Embedding(2, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        ...(1)-(23)
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=1024, out_features=2, bias=True)
)
```
### 3.6.4预测代码
```
for i in tqdm(range(len(names))):
    try:
        res=es_handler.search_by_ids([names[i]])#1.es检索录音文本
        lines=model_utils.get_es_dialog_txt_single(res[str(names[i])]) #2.将文本转为能处理的形式        
        q,lines = model_utils.cut_dialog_doc(lines,model_utils.label_num,question_model,tokenizer,device='cuda')#3.返回问题类+段落

    except Exception as e:
        tmpdf=pd.DataFrame()
        tmpdf['order']=[i]
        tmpdf['receipt_id']=[names[i]]
        for key in model_utils.form_quesiton_ids:
            tmpdf[key]=['']
        tmpdf[1]=['no_asr']
        answer_df=pd.concat([answer_df,tmpdf])
        print(e)
        continue
    data,answer_dict=model_utils.prepare_paragraph(q)#根据问题对象q将文本转化为符合模型输入的格式，结果为data
    args.version_2_with_negative=False#模型推理用到
    res,preds=model_utils.predict(args, answer_model, tokenizer2,data=data)#preds:原始预测结果，res:整理的预测结果，eg:OrderedDict([(1, '50到100'),(2, '一公斤以内'),(3, '几十'),(4, '顺丰'),...])
	q_new,answer_dict=model_utils.get_answer(q,preds,answer_dict,answer_thresh=0.2)#q_new 被填充的q  
    answer_dict=model_utils.get_transformed_answer(q_new,answer_dict)#answer_dict被转义
    tmpdf=pd.DataFrame()
    tmpdf['order']=[i]
    tmpdf['receipt_id']=[names[i]]
    
    for key in model_utils.form_quesiton_ids:
        
        tmpdf[key]=[answer_dict[key]]
        
    if '文件' in tmpdf[8].iloc[0].split(';'):
        tmpdf[2].iloc[0]=tmpdf[2].iloc[0]+';500g-1公斤（1000g）-特惠送特快送'
        tmpdf[3].iloc[0]=tmpdf[3].iloc[0]+';50-100'
    answer_df=pd.concat([answer_df,tmpdf])
```
# 4 架构
	![[Pasted image 20230910224539.png]]
## 4.1 asr话务录音转文本
前置流程 : ASR录音转文本。
京东科技每天将质检录音转化为文本，系统侧通过接口读取对应的录音转文本结果存储到ElasticSearch。
ASR录音转文本存储结果如图所示：
	![](https://apijoyspace.jd.com/v1/files/yMRw0zHTthVrfyUBIX2j/link)
## 4.2 机器质检问题答案识别
### 4.2.1 读取ES文本，文本预处理
从ElasticSearch上读取质检文本， 再将其中的接口返回结果处理成客服和顾客问答的可用形式
**ElasticSearch链接：**
#### ES【测试】
spring.elasticsearch.rest.uris=http://es-nlb-es-4l2urvd5ft.jvessel-open-hb.jdcloud.com:9200
spring.elasticsearch.rest.username=open-forecast-adjustment
spring.elasticsearch.rest.password=91110108MA01E3926N_jdlsjcpbai
spring.elasticsearch.index.asrAudioIndexName=human_asr_receipt_text
索引：human_asr_receipt_text
#### 预处理后文本如图所示：
![](https://apijoyspace.jd.com/v1/files/1PrXLMjkWRh7lQCaU0br/link)
##### 4.2.2 机器质检模型自动识别表单问题答案
通过读取前置流程中的录音转文本并将ASR返回后系统侧存储到ES上的文本，并进行文本预处理，处理成机器质检模型可用的输入形式，然后通过机器质检问题识别和答案识别模型识别到质检表单问题和答案，模型结果存储到Hive，通过定时任务部署到线上，小流量分流到机器质检部分每天写入机器质检结果。
##### 4.2.3 ChatGPT结果融合
为提高机器质检准确率，可以将我们的问题答案识别模型结果和ChatGPT结果融合，输出重合的结果。
目前ChatGPT是从文本中抽取出问题和对应问题答案的文本，我们的机器质检模型是从文本中识别到问题和对应问题答案的表单选项， 将ChatGPT结果文本转义后映射到答案表单选项，和我们的机器质检模型结果融合输出， 提高机器质检准确率。目前ChatGPT交付还在沟通中，可以先上线我们的问题答案识别模型结果，后续分期迭代，融合ChatGPT结果，逐步完善。


# 5.指标
|指标维度|机器质检质检准确率（转义后）|ChatGPT质检准确率（未转义，未转义需要转义映射成表单答案，会有准确率损失）|
|单量|92%|86.4%|
|重量|94%|95.5%|
|价格|96%|88.6%|
|竞品|86%|81.8%|
|时效|88%|88.6%|
|特殊需求|90%|95.5%|
|邮寄物|86%|95.5%|
人工质检当前准确率： 85% -95%
智能质检当前准确率： 上述已评估50条样本质检准确率，待评估1000条样本智能质检准确率
智能质检后续使用： 一次质检，二次质检，仲裁
