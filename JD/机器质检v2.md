# 1.答案抽取V2-燃灯
```
pretrain_path = '/home/wangran108/question-answering/output_dir'
#缺模型文件，缺训练,训练代码：run_seq2seq_qa_no_trainer.py ？

w=torch.load("/home/wangran108/code/Fengshenbang-LM/fengshen/examples/qa_t5/ckpt/last.ckpt/model")
#https://github.com/IDEA-CCNL/Fengshenbang-LM/tree/main/fengshen/examples/qa_t5/run_finetune.sh微调？
```
## T5模型的原理
T5模型（Transformers-based Text-to-Text Transfer Transformer）是由Google Brain团队在2019年提出的一种基于Transformer结构的序列到序列（Seq2Seq）模型，其主要特点是将多种NLP任务（如翻译、摘要、问答等）转化为一个统一的框架下进行训练。

T5模型包括编码器和解码器两个部分。编码器是一种多层的Transformer编码器，用于将输入的自然语言文本进行编码表示。解码器也是一种多层的Transformer解码器，用于将编码器输出的表示转换为目标自然语言文本。在训练中，T5模型采用了文本到文本的框架，即将多种NLP任务转化为对输入文本进行变换，从而得到对应的输出文本的形式进行训练。
![[Pasted image 20231004223012.png]]
## T5模型的优劣势
### T5模型的主要优势在于：
具有可扩展性：T5模型可以处理多种NLP任务，并且可以通过微调来适应不同的应用场景，具有良好的可扩展性。
模型参数少：相比其他语言生成模型（如GPT-2、BERT等），T5模型的参数数量相对较少，训练速度更快，且可以在相对较小的数据集上进行训练。
优秀的性能表现：T5模型在多种NLP任务中都表现出了非常优秀的性能，如在GLUE数据集上取得了目前最好的结果。
### T5模型的主要劣势在于：
训练时间较长：由于T5模型使用了大量的Transformer结构，在训练时需要大量的计算资源和时间。
模型的可解释性不足：T5模型由于结构较为复杂，参数数量庞大，导致其内部机制不够透明，解释性较差，难以理解其决策过程。
## T5模型的应用案例
机器翻译：T5模型可以将一种语言翻译成另一种语言，例如将英语翻译成中文。
文本摘要：T5模型可以将长篇文章转化为简洁的摘要，提取文章的关键信息。

## 燃灯T5预训练
https://zhuanlan.zhihu.com/p/590061741
谷歌T5预训练：[T5 模型：NLP Text-to-Text 预训练模型超大规模探索 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/88438851)
![[Pasted image 20231005225008.png]]
- Transformer Encoder-Decoder 模型；
- BERT-style 式的破坏方法；
- Replace Span 的破坏策略；
- 15 %的破坏比；
- 3 的破坏时小段长度。
## 燃灯T5结构
```
MT5ForConditionalGeneration(
  (shared): Embedding(32601, 1024)
  (encoder): T5Stack(
    (embed_tokens): Embedding(32601, 1024)
    (block): ModuleList(
      (0): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=1024, out_features=1024, bias=False)
              (k): Linear(in_features=1024, out_features=1024, bias=False)
              (v): Linear(in_features=1024, out_features=1024, bias=False)
              (o): Linear(in_features=1024, out_features=1024, bias=False)
              (relative_attention_bias): Embedding(32, 16)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
              (wo): Linear(in_features=2816, out_features=1024, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (gelu_act): NewGELUActivation()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      ...
      (23)
    )
    (final_layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
   (decoder): T5Stack(
    (embed_tokens): Embedding(32601, 1024)
    (block): ModuleList(
      (0): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=1024, out_features=1024, bias=False)
              (k): Linear(in_features=1024, out_features=1024, bias=False)
              (v): Linear(in_features=1024, out_features=1024, bias=False)
              (o): Linear(in_features=1024, out_features=1024, bias=False)
              (relative_attention_bias): Embedding(32, 16)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=1024, out_features=1024, bias=False)
              (k): Linear(in_features=1024, out_features=1024, bias=False)
              (v): Linear(in_features=1024, out_features=1024, bias=False)
              (o): Linear(in_features=1024, out_features=1024, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
              (wo): Linear(in_features=2816, out_features=1024, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (gelu_act): NewGELUActivation()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      ...(23)
    ) 
    (final_layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (lm_head): Linear(in_features=1024, out_features=32601, bias=False)
) 
```
[【大神笔记】一文看懂Google推出的大一统模型—T5 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/589869911)
[Sentence T5, T5, BERT, GPT区别 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/409771204)

## 燃灯T5配置参数
```
finetune_t5_cmrc.py --max_epochs 2 --gpus 4 --num_nodes 1 --strategy ddp --default_root_dir /home/wangyulu20/work1/NLP --save_ckpt_path /home/wangyulu20/work1/NLP/ckpt --save_top_k 5 --every_n_train_steps 100 --monitor val_rougeL_fmeasure --mode max --save_last --check_val_every_n_epoch 1 --num_workers 1 --dataloader_workers 1 --replace_sampler_ddp False --accumulate_grad_batches 2 --formator t5style --filename 'model-{epoch:02d}-{val_loss:.4f}-{val_rougeL_fmeasure:.3f}' --precision 16 --pretrained_model_path /home/wangyulu20/work1/inspection/mqi/randengt5 --tokenizer_type t5_tokenizer --learning_rate 1e-4 --weight_decay 1e-2 --warmup_ratio 0.1 --scheduler_type polynomial --min_learning_rate 1e-5 --train_batchsize 2 --val_batchsize 2 --train_file /home/wangyulu20/work1/inspection/mqi/sample7.json --val_file /home/wangyulu20/work1/inspection/mqi/sample7.json --max_seq_length 512 --max_knowledge_length 425 --max_target_length 128
```

## 燃灯预测demo
详情见：
https://github.com/IDEA-CCNL/Fengshenbang-LM/tree/c8fb7b8437843ea13fa9d147ce86c4592fa21237/fengshen/examples/qa_t5
```
config = MT5Config.from_pretrained(pretrain_path)
tokenizer=T5Tokenizer.from_pretrained(pretrain_path)
model=MT5ForConditionalGeneration.from_pretrained(pretrain_path)
model=model.to('cuda:0')
all_tokenized=[]
all_plain_text=[]
for q in q_set:
    input_text='客服:您好，我是京东物流 ... 顾客:行吧。可以。'
    plain_text = (
                "question:"
                + q
                + "context:"
                + input_text
            )
    all_plain_text.append(plain_text)
    l_text = len(plain_text)
    res_prefix = tokenizer.encode("answer:", add_special_tokens=False)
    l_rp = len(res_prefix)
    tokenized = tokenizer.encode(
        plain_text,
        add_special_tokens=False,
        truncation=True,
        max_length=1024,
        padding='max_length'
    )
    tokenized += res_prefix
    # add maskid
    mask_id = tokenizer.convert_tokens_to_ids("<extra_id_0>")
    tokenized.append(mask_id)
    tokenized.append(config.eos_token_id)
    all_tokenized.append(tokenized)
input_ids=torch.tensor(all_tokenized).to('cuda:0')
with torch.no_grad():
    pred_ids = model.generate(input_ids=input_ids,num_beams=1,top_k=6,do_sample=True,temperature=1,num_return_sequences=1)
    res=tokenizer.batch_decode(pred_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)
    res=[each.lstrip("<extra_id_0>") for each in res]
    torch.cuda.empty_cache()
```
# 2.转义前置模型V2-GPT2-base-ch
```
model_dir = "/home/wangran108/code/FlagAI-master/examples/gpt2_title_generation/100006"

model_save_path='/home/wangran108/code/FlagAI-master/examples/gpt2_title_generation/pytorch_model4.bin'
```
作用于答案抽取后，转义前，只作用于单量，重量，价值这类数值区间的答案，为使待转义的文本规范化。
## GPT2
1. https://blog.csdn.net/weixin_44750512/article/details/129387591
2. https://baijiahao.baidu.com/s?id=1642813680348638520&wfr=spider&for=pc
3. https://baijiahao.baidu.com/s?id=1642914993515919916&wfr=spider&for=pc
# 3.大模型版本
```#train.sh
export XDG_CACHE_HOME=/home/wangran108
export TRANSFORMERS_OFFLINE=1
export OMP_NUM_THREADS=2
# /opt/conda/bin/python
accelerate launch finetune.py \
    --base_model '/home/wangran108/baichuan' \
    --data_path '/home/wangran108/code/machine_quality_check/sample7.json' \
    --output_dir './baichuan_lora2' \
    --batch_size 32 \
    --micro_batch_size 4 \
    --num_epochs 1 \
    --learning_rate 1e-4 \
    --cutoff_len 1024 \
    --val_set_size 0.01 \
    --lora_r 8 \
    --lora_alpha 16 \
    --lora_dropout 0.05 \
    --lora_target_modules '[W_pack]' \
> log.log 2>&1 &
```
