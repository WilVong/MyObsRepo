# 1.答案抽取V2-燃灯
```
pretrain_path = '/home/wangran108/question-answering/output_dir'
#缺模型文件，缺训练,训练代码：run_seq2seq_qa_no_trainer.py ？

w=torch.load("/home/wangran108/code/Fengshenbang-LM/fengshen/examples/qa_t5/ckpt/last.ckpt/model")
#https://github.com/IDEA-CCNL/Fengshenbang-LM/tree/main/fengshen/examples/qa_t5/run_finetune.sh微调？
```
## T5模型的原理
T5模型（Transformers-based Text-to-Text Transfer Transformer）是由Google Brain团队在2019年提出的一种基于Transformer结构的序列到序列（Seq2Seq）模型，其主要特点是将多种NLP任务（如翻译、摘要、问答等）转化为一个统一的框架下进行训练。

T5模型包括编码器和解码器两个部分。编码器是一种多层的Transformer编码器，用于将输入的自然语言文本进行编码表示。解码器也是一种多层的Transformer解码器，用于将编码器输出的表示转换为目标自然语言文本。在训练中，T5模型采用了文本到文本的框架，即将多种NLP任务转化为对输入文本进行变换，从而得到对应的输出文本的形式进行训练。
![[Pasted image 20231004223012.png]]
## T5模型的优劣势
### T5模型的主要优势在于：
具有可扩展性：T5模型可以处理多种NLP任务，并且可以通过微调来适应不同的应用场景，具有良好的可扩展性。
模型参数少：相比其他语言生成模型（如GPT-2、BERT等），T5模型的参数数量相对较少，训练速度更快，且可以在相对较小的数据集上进行训练。
优秀的性能表现：T5模型在多种NLP任务中都表现出了非常优秀的性能，如在GLUE数据集上取得了目前最好的结果。
### T5模型的主要劣势在于：
训练时间较长：由于T5模型使用了大量的Transformer结构，在训练时需要大量的计算资源和时间。
模型的可解释性不足：T5模型由于结构较为复杂，参数数量庞大，导致其内部机制不够透明，解释性较差，难以理解其决策过程。
## T5模型的应用案例
机器翻译：T5模型可以将一种语言翻译成另一种语言，例如将英语翻译成中文。
文本摘要：T5模型可以将长篇文章转化为简洁的摘要，提取文章的关键信息。

## 燃灯T5预训练
https://zhuanlan.zhihu.com/p/590061741

## 燃灯T5结构
```
MT5ForConditionalGeneration(
  (shared): Embedding(32601, 1024)
  (encoder): T5Stack(
    (embed_tokens): Embedding(32601, 1024)
    (block): ModuleList(
      (0): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=1024, out_features=1024, bias=False)
              (k): Linear(in_features=1024, out_features=1024, bias=False)
              (v): Linear(in_features=1024, out_features=1024, bias=False)
              (o): Linear(in_features=1024, out_features=1024, bias=False)
              (relative_attention_bias): Embedding(32, 16)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
              (wo): Linear(in_features=2816, out_features=1024, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (gelu_act): NewGELUActivation()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      ...
      (23)
    )
    (final_layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
   (decoder): T5Stack(
    (embed_tokens): Embedding(32601, 1024)
    (block): ModuleList(
      (0): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=1024, out_features=1024, bias=False)
              (k): Linear(in_features=1024, out_features=1024, bias=False)
              (v): Linear(in_features=1024, out_features=1024, bias=False)
              (o): Linear(in_features=1024, out_features=1024, bias=False)
              (relative_attention_bias): Embedding(32, 16)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=1024, out_features=1024, bias=False)
              (k): Linear(in_features=1024, out_features=1024, bias=False)
              (v): Linear(in_features=1024, out_features=1024, bias=False)
              (o): Linear(in_features=1024, out_features=1024, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
              (wo): Linear(in_features=2816, out_features=1024, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (gelu_act): NewGELUActivation()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      ...(23)
    ) 
    (final_layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (lm_head): Linear(in_features=1024, out_features=32601, bias=False)
) 
```
[【大神笔记】一文看懂Google推出的大一统模型—T5 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/589869911)
[Sentence T5, T5, BERT, GPT区别 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/409771204)
# 2.转义模型V2-GPT2-base-ch
```
model_dir = "/home/wangran108/code/FlagAI-master/examples/gpt2_title_generation/100006"

model_save_path='/home/wangran108/code/FlagAI-master/examples/gpt2_title_generation/pytorch_model4.bin'
```