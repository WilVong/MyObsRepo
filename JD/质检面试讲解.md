# 1概述
## 1.1背景
我们部门为了做b端的商家新签，需要将排序后的商家联系方式分发到外呼人员中，外呼人员会去询问发单量，价值，时效性等关键信息，并填写表单，质检要做的就是根据录音文本验证表单填写的准确性
## 1.2流程
整体质检流程分为四步：
1.问题抽取：将客服和客户的每句话分类，对应为质检问题的某个类别，或是无类别
2.段落切分：将整个文本切分为若干个文本块，每个质检问题对应一块，切分大体是按一个问题的询问触发到一个问题的回答结束为依据分割的
3.答案抽取：在每个质检问题对应的文本块中找到正确的答案，比如单量是xx，时效性是xx
4.转义：质检要做的事就是验证外呼人员填写表单的正确性，而外呼人员填写的表单中每个问题是固定的枚举值，所以要将抽取出的答案映射为表单中的标准格式，比如价格中：十几块，一百块映射为0-50元，100-200元这种标准表单选项

v0版的方案都是用规则做的，无论是问题抽取还是答案抽取，每个质检类都有多个正则表达式去匹配，但带来的问题是有小部分case是无法被涵盖到的，而且这些case数量多种类杂，不能用有限个正则表达式解决，从而转为模型识别

样本：1.2w个
原始标注：
![[Pasted image 20231203170630.png]]
需要转成模型输入的格式

## 1.3模型
### 1.3.1问题抽取v1
科大讯飞和哈工大合作预训练模型 
[https://github.com/ymcui/Chinese-BERT-wwm](https://github.com/ymcui/Chinese-BERT-wwm)
【ps:wwm：bert预训练MLM时Mask改为分词，从而字的embedding学到词信息】
用其做多标签分类
模型：bert-base参数：12-layer, 768-hidden, 12-heads, 110M 参数
使用bert-large：24-layer, 1024-hidden, 16-heads, 330M参数
样本：multi-hot 
实现Dataset类__getitem__方法
```
return {
	'ids': torch.tensor(ids, dtype=torch.long),
	'mask': torch.tensor(mask, dtype=torch.long),
	'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),
	'targets': torch.tensor(self.targets[index], dtype=torch.float)} 
```
损失：sigmoid+二进制交叉熵（BCE）= nn.BCEWithLogitsLoss
微调训练：batch=32 l_r=2e-5 max_len=120 epoch 4 时间 一小时左右
bertbase训练显存就十几G
指标：召回率，metrics.f1_score 0.97 ps:多分类指标
### 1.3.2段落分割
v0:正则匹配每句话，触发每类问题的询问，分割范围为第一次触发这个问题的句子到下一个触发其他问题的句子
v1:
1.遍历文本list，对每句话预处理
2.送入分类模型得到各类概率
3.每类概率超过阈值0.5时，将这句话索引加入各类别key设置的list
4.兜底：list中没有句子索引，一句话最大概率不是无类别时，也将这句话放入对应key的list
5.每个类别的句子索引取位置最后的那句话，排序索引
6.划分段落到问题对象q，划分依据：问题触发行到下一个问题触发
### 1.3.3答案抽取：MRC
模型：
macbert large：Bert预训练MLM的改进：
1.mask为词级别，用Ngram mask，unigram到4-gram占比：40%，30%，20%，10%
2.不是用[MASK]替换，而是基于向量相似度替换n-gram中的词
3.15%比例Mask，其中8：1：1为，相似替换：随机替换：保留
机器阅读理解：给定问题和上下文，找到对应答案
对于片段提取，目标是预测答案开始结束位置，Bert输出n * d的特征，送入开始索引矩阵，结束索引矩阵（d * 2），经过行softmax得到每个位置是开始和结束的概率，任意两位置i,j的特征拼起来送入匹配矩阵(1 * 2d)再Sigmoid得到两位置匹配的概率
三个分类全部为交叉熵损失，其标签Y_start,Y_end为长度为句子大小onehot的向量，Y_s,e为匹配标签，是个n * n的矩阵
测试时，找到多个开始索引结束索引，再用索引匹配矩阵找到最大概率匹配的两个位置
参数：24层，head=16 hidden=1024
样本：question+context
训练：batch=4(说8也行) l_r=2e-5 max_len=384 epoch=2(说3也行) 1个多小时
预测：
指标：rouge

### 1.3.4 T5
Transformer的编码解码结构 24层编码 解码，1024隐层 head=16  784M
预训练：
Replace Span代替Mask，长度为3，15%破坏比例
多任务训练，每类任务包含特有的提示词
两步训练：1.MLM，15%比例mask，一定会mask标注关键信息，做预训练
2.提示词微调
构造样本：问题提示词+分割context
微调：epoch=2 gpu=2 l_r=1e-4 batch=4 max_len=512 1小时
评估：rouge提升5%

### 1.3.5 GPT2
单向Transformer，适用于生成任务 增加数据集，参数（48层，隐向量1600），bpe分词 扩词表 1542M=1.5B
可加提示词
构造样本：收集case，数据增强扩量(同义替换)->2k 提示词+context 
微调：l_r=e-4 epoch=2 batch=4 gpu=2 max_len=512
评估：和转义一起评估acc 准确率提升3%
### 1.3.6 chatglm2
构造样本：问题提示词+全文本 label:正确标注 场景单一，纯对话样本，无通用数据 
结构：28个glm层（类transformer），multi-query注意力，旋转位置编码，swiGLU，RMSNorm归一化
参数：lora微调 单卡 lr=1e-5 maxlen=1024 batch=16 fp16 epoch=4
部署：单卡 8bit
评估：rouge提升4% 准确率提升2%
节约3/4人工质检员成本
