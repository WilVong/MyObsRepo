一.传统大模型微调：
1.微调所有参数
2.调整部分参数（冻结部分层）
3.对模型尾部加一些层，训练新加层

4.训练所需显存：6* BatchSize* 参数量（近似经验公式）
Adam训练法下约为6
显存里面到底存了哪些数据：
模型：6b个float32 浮点数 Adam梯度下降法： 存前向结果 存之前步骤的梯度结果
eg:6b参数量，则为6* 6b* 10=360G显存
代码：（finetune_freeze.ipynb）
    #model加载好的大模型
    #层数
    layer_num=len([ p for p in model.parameters()])
    freeze_para=0
    train_para=0
    for i,p in enumerate(model.parameters()):
        print (i)
        print (p)
        #把前面所有的层数都给冻结住
        if i<layer_num-2:
            #冻结可调参数
            p.requires_grad = False
            freeze_para+=mul(p.shape)
        else:#倒数后两层可以训练
            train_para+=mul(p.shape)
    print ("冻结参数:",freeze_para)
    print ("可训练参数:",train_para)
    print ("所有参数:",freeze_para+train_para)

5.因为显存的限制： 模型可训练参数不能太多；Batchsize不能太大 ；Max_length 不能太大
6.经验：
Maxlength：一般会统计平均长度（去除异常值） 平均长度* 1.5 确定好训练的参数 
Batchsize理论上来说，越大越好，知道榨干gpu性能
7.分词代码
def preprocess(tokenizer, config, example, max_seq_length):
    #问题
    prompt = example["context"]
    #答案
    target = example["target"]
    #问题分词
    prompt_ids = tokenizer.encode(prompt, max_length=max_seq_length, truncation=True)
    #答案分词
    target_ids = tokenizer.encode(
        target,
        max_length=max_seq_length,
        truncation=True,
        add_special_tokens=False)
    input_ids = prompt_ids + target_ids + [config.eos_token_id]
    #input_ids:问题分词+答案分词  seq_len:问题长度
    return {"input_ids": input_ids, "seq_len": len(prompt_ids)}

直接加载分词器文件：
	tokenizer = transformers.AutoTokenizer.from_pretrained("E:\code\chatglm\chatglm2",trust_remote_code=False)
8.分词相关
Bpe分词：常见组合的字符串拼接在一起
Sentencepiece：Google出现分词工具 它可以实现bpe 把所有的文本都转成utf-8 ，所以有可能： 1.几个汉字对应一个token 2.两个token对应一个汉字

9.训练数据构建：
输出：-100 -100 -100 e f g h <end>（问题不需要预测，输出部分-100填充，代表不预测）
模型：              gpt/glm/...
输入：a       b        c    d e f g  h  (问题+答案，a-d问题)

代码：
def data_collator(features: list) -> dict:
    len_ids = [len(feature["input_ids"]) for feature in features]
    longest = max(len_ids)
    input_ids = []
    labels_list = []
    for ids_l, feature in sorted(zip(len_ids, features), key=lambda x: -x[0]):
        #问题+答案
        ids = feature["input_ids"]
        #问题长度
        seq_len = feature["seq_len"]
        #-100特殊字符，表示不预测
        # [-100] * (seq_len - 1) 问题部分是不需要预测的
        #ids[(seq_len - 1) :] 预测答案
        #[-100] * (longest - ids_l)  补零位置不需要预测，超过最大长度会被截断
        labels = (
            [-100] * (seq_len - 1) + ids[(seq_len - 1) :] + [-100] * (longest - ids_l)
        )
        ids = ids + [tokenizer.pad_token_id] * (longest - ids_l)
        _ids = torch.LongTensor(ids)
        labels_list.append(torch.LongTensor(labels))
        input_ids.append(_ids)
    input_ids = torch.stack(input_ids)
    labels = torch.stack(labels_list)
    return {
        "input_ids": input_ids,
        "labels": labels,
    }

10.【直接】微调模型参数，使用场景：
	1.数据量大 token数量>=可调参数 
	2.机器算力足 显存大小决定能否训练 显卡flops决定训练时间 
	3.场景非常垂直
11.大模型调参难点，为什么很少直接微调？ 
	1.参数多，显存不容易放下 
	2.参数多，需要对应更大数据 
	3.参数多，不容易收敛 
	4.参数多，调参时间过长
做大模型： 一半技术 一半运气
12.参数高效微调方法（Parameter-Efficient Fine-Tuning，PEFT） 
• Prefix-Tuning / Prompt-Tuning：在模型的输入或隐层添加 k 个额外可训练的前缀 tokens（这些前缀是连 续的伪 tokens，不对应真实的 tokens），只训练这些前缀参数；
• Adapter-Tuning：将较小的神经网络层或模块插入预训练模型的每一层，这些新插入的神经模块称为 adapter（适配器），下游任务微调时也只训练这些适配器参数；
• LoRA：通过学习小参数的低秩矩阵来近似模型权重矩阵 W的参数更新，训练时只优化低秩矩阵参数。

三件套出发点：为了节约内存
13.P-Tuning举例：
问题：今天能否打篮球
Prompt+问题：今天天晴，温度25度。今天能否打篮球 
Prefix+问题：a b c d e今天能否打篮球 答案：今天可以打篮球
Prompt加真实提示词，Prefix加虚拟token

14.待训练参数要用float32，如果用float16 可能会出现 nan inf
	![[屏幕截图 2023-08-25 002954.jpg]]



