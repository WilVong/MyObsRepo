1.旋转自编码 RoPE
	![[Pasted image 20230902014224.png]]
	![[Pasted image 20230902014305.png]]
	![[Pasted image 20230902014117.png]]
	![[Pasted image 20230902014358.png]]
	![[Pasted image 20230902014426.png]]
	![[Pasted image 20230902014634.png]]
2.pre norm &post norm
	![[Pasted image 20230902201644.png]]
	post norm更好：
	![[Pasted image 20230902202119.png]]

3.FlashAttention
	![[Pasted image 20230902202544.png]]
	将矩阵分解为小块送入SRAM计算
	优化前：算一次大型矩阵 
	优化后：算多次小矩阵 计算量整体是变多的 但是数据读取快，整体还是变快
	![[Pasted image 20230902202842.png]]
4.Multi Query Attention：多头注意力简化版
	![[Pasted image 20230902203117.png]]
5.模型量化
深度学习领域提出了一系列的模型压缩与加速方法： 
•剪枝（Parameter pruning） （少用）
•低秩分解（Low-rank factorization） （多用）
•知识蒸馏（Knowledge distillation）-小模型代替大模型 （少用）
•量化（quantization）-少量字节代替高精度float32（多用）

量化计算：
	![[Pasted image 20230902203913.png]]
	方法：1.直接缩放于attention层和dense层 2.Post-training-quantization 3.QAT（2，3理论中，不常用）


