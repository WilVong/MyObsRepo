# 1.损失函数
## 1.1  torch.nn.BCELoss()（二进制交叉熵)
-  基础的损失函数 BCE （Binary cross entropy）
```
class torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction=‘elementwise_mean’)
```
-  pytorch中，表示求一个二分类的交叉熵:
![[Pasted image 20230920015006.png]]
  当参数reduce设置为 True，且参数size_average设置为True时，表示对交叉熵求均值，当size_average设置为Flase时，表示对交叉熵求和。参数weight设置的是 w_n​ ​，其是一个tensor, 且size与批量数一样(不设置时可能都为1)。目标值 y的范围是0-1之间。输入输出的维度都是 ，(N,* ),N是批量数，* 表示目标值维度。
- 使用交叉熵作为损失函数后，反向传播的梯度不在于sigmoid函数的导数有关了。这就从一定程度上避免了梯度消失。而使用MSE和sigmoid将导致梯度消失。

-  分类问题一般又分为：二分类任务、多分类任务和多标签分类任务
二分类任务：输出只有0和1两个类别；——sigmoid+BCE loss
多分类任务：一般指的是输出只有一个标签，类别之间是互斥的关系；——softmax
多标签分类任务：输出的结果是多标签，类别之间可能互斥也可能有依赖、包含等关系。——sigmoid
### 1.1.1二分类
在面对二分类的问题时，预测值经sigmoid 后数值在0-1区间。特征项 [x_0,x_1],标签要么是[0,1],要么[1,0]
面对二分类问题时，CELoss是Softmax + BCELoss。
### 1.1.2多分类(单标签/多标签)
输出矩阵经sigmoid，与标签计算交叉熵
```
# 预测值
predict = torch.Tensor([[0.5796, 0.4403, 0.9087],
                        [-1.5673, -0.3150, 1.6660]])
# 真实值
bec_target = torch.Tensor([[0, 0, 1],
                           [1, 0, 0]])
# 1. CELoss
# 真实值
ce_target = torch.tensor([2, 0])
ce_loss = torch.nn.CrossEntropyLoss()
print('ce_loss:', ce_loss(predict, ce_target)) 

# 2.sigmoid + BCELoss
soft_input = torch.nn.sigmoid(dim=-1)#多标签为sigmoid
soft_out = soft_input(predict)

bce_loss = torch.nn.BCELoss()
print('bce_loss:', bce_loss(soft_out, bec_target)) 
```
