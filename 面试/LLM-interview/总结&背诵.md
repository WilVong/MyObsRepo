# 一 LLM基础面
## 1.主流开源体系
1. GPT（Generative Pre-trained Transformer）系列：OpenAI的 Transformer架构 
2. BERT（Bidirectional Encoder Representations from Transformers）：谷歌的 在无标签文本上进行预训练，然后在特定任务上进行微调
3. XLNet：CMU和Google Brain的  Transformer架构  自回归预训练 
4. RoBERTa：Facebook的  Transformer架构  BERT的改进，更大规模数据和更长训练时间
5. T5（Text-to-Text Transfer Transformer）：Google的  Transformer架构  多任务预训练模型
## 2.语言模型结构分类
![[Pasted image 20231101010848.png]]
1. Encoder-Decoder（Transformer）: Encoder 部分是 Masked Multi-Head Self-Attention，Decoder 部分是 Casual Multi-Head Cross-Attention 和 Casual Multi-Head Self-Attention 兼具。
2. 因果语言模型（Causal Language Model, CLM）: 即Transformer的Decoder，比如GPT。也叫自回归语言模型（Auto-Regressive Language Models）
3. 掩蔽语言模型（Masked Language Model, MLM）: 即Transformer的Encoder，学习过程，能看到待预测词的前后内容，如BERT、ERNIE、ALBERT、RoBERTa
4. 前缀语言模型（Prefix language model）：如UniLM。前缀语言模型在抽取输入文本特征时用了 Fully-Visible Mask（Encoder 用的掩码，能看到「过去」和「未来」）；生成本文部分则与自回归语言模型一样，只看到左侧
## 3.涌现能力的原因
1. 数据量的增加
2. 计算能力的提升
3. 模型架构的改进
4. 预训练和微调的方法：无标签数据上预训练，特定任务上进行微调
## 4.常见大模型LLM的架构
 Transformer架构  自注意力机制  多头注意力 前馈神经网络  预训练和微调
# 二 LLM进阶面
## 1.什么是 LLMs 复读机问题
概念：
模型倾向于复制输入的文本，或频繁重复相同的句子或短语。
使得模型的输出缺乏多样性，创造性
原因：
1. 数据偏差：训练数据中存在大量的重复文本/特定的句子或短语出现频率较高
2. 训练目标的限制：LLM的训练通常基于 【自监督学习 】方法，通过预测下一个词或掩盖词来学习语言模型。这样的训练目标可能使得模型倾向生成与输入相似的文本
3. 缺乏多样性的训练数据
对策：
1. 多样性训练数据
2. 引入噪声：引入随机性或噪声。如采样不同的词或短语，或者引入随机的变换操作，以增加生成文本的多样性。
3. 温度参数调整：温度参数较高时，softmax的输出结果将更加平滑，这将使生成结果更加多样化
4. 后处理和过滤：去除重复的句子或短语，提高生成文本的质量 多样性。
