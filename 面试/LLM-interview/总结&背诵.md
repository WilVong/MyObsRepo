# 一 LLM基础面
## 1.主流开源体系
1. GPT（Generative Pre-trained Transformer）系列：OpenAI的 Transformer架构 
2. BERT（Bidirectional Encoder Representations from Transformers）：谷歌的 在无标签文本上进行预训练，然后在特定任务上进行微调
3. XLNet：CMU和Google Brain的  Transformer架构  自回归预训练 
4. RoBERTa：Facebook的  Transformer架构  BERT的改进，更大规模数据和更长训练时间
5. T5（Text-to-Text Transfer Transformer）：Google的  Transformer架构  多任务预训练模型
## 2.语言模型结构分类
![[Pasted image 20231101010848.png]]
1. Encoder-Decoder（Transformer）: Encoder 部分是 Masked Multi-Head Self-Attention，Decoder 部分是 Casual Multi-Head Cross-Attention 和 Casual Multi-Head Self-Attention 兼具。
2. 因果语言模型（Causal Language Model, CLM）: 即Transformer的Decoder，比如GPT。也叫自回归语言模型（Auto-Regressive Language Models）
3. 掩蔽语言模型（Masked Language Model, MLM）: 即Transformer的Encoder，学习过程，能看到待预测词的前后内容，如BERT、ERNIE、ALBERT、RoBERTa
4. 前缀语言模型（Prefix language model）：如UniLM。前缀语言模型在抽取输入文本特征时用了 Fully-Visible Mask（Encoder 用的掩码，能看到「过去」和「未来」）；生成本文部分则与自回归语言模型一样，只看到左侧
## 3.涌现能力的原因
1. 数据量的增加
2. 计算能力的提升
3. 模型架构的改进
4. 预训练和微调的方法：无标签数据上预训练，特定任务上进行微调
## 4.常见大模型LLM的架构
 Transformer架构  自注意力机制  多头注意力 前馈神经网络  预训练和微调
# 二 LLM进阶面
## 1 什么是 LLMs 复读机问题
概念：
模型倾向于复制输入的文本，或频繁重复相同的句子或短语。
使得模型的输出缺乏多样性，创造性
原因：
1. 数据偏差：训练数据中存在大量的重复文本/特定的句子或短语出现频率较高
2. 训练目标的限制：LLM的训练通常基于 【自监督学习 】方法，通过预测下一个词或掩盖词来学习语言模型。这样的训练目标可能使得模型倾向生成与输入相似的文本
3. 缺乏多样性的训练数据
对策：
1. 多样性训练数据
2. 引入噪声：引入随机性或噪声。如采样不同的词或短语，或者引入随机的变换操作，以增加生成文本的多样性。
3. 温度参数调整：温度参数较高时，softmax的输出结果将更加平滑，这将使生成结果更加多样化
4. 后处理和过滤：去除重复的句子或短语，提高生成文本的质量 多样性。
## 2 llama 输入句子长度理论上可以无限长吗
理论上LLM可以处理任意长度输入，但实际上存在限制和挑战。
1. 计算资源：更多的计算资源，包括内存和计算时间。
2. 模型训练和推理：训练阶段，处理长句子可能会导致梯度消失或梯度爆炸的问题，在推理阶段，生成长句子可能会增加模型的错误率和生成时间。 
3. 上下文建模：LLMs是基于上下文建模的模型，长句子的上下文可能会更加复杂和深层。
对策：
采用分块的方式处理长句，分成多个较短的片段处理。
增加计算资源、优化模型结构和参数设置，使用更高效的推理算法。
## 3 什么情况用Bert模型，什么情况用LLaMA、ChatGLM
1. Bert模型：适用于通用文本处理任务：文本分类、命名实体识别、语义相似度计算等。
2. LLaMA模型：？
3. ChatGLM模型：面向对话生成，适用于聊天机器人、智能客服等对话系统。需要处理对话上下文、生成多轮对话
限制：
1.数据可用性：不同模型可能需要不同类型和规模的数据
2.计算资源：大模型通常需要更多的计算资源和存储空间。
3.预训练和微调：大模型通常需要进行预训练和微调才能适应特定任务和领域。
## 4 各个专业领域是否需要各自的大模型来服务
需要
原因：
 1. 领域特定知识：不同领域拥有各自特定的知识和术语
 2. 语言风格和惯用语：各个领域有自己独特的语言风格和惯用语。专门针对某个领域进行训练的大模型可以更好地【掌握该领域的语言特点】，生成更【符合该领域要求】的文本。
 3. 领域需求的差异：eg，金融领域可能更关注数字和统计数据的处理，而法律领域可能更关注法律条款和案例的解析。
 4. 数据稀缺性：某些领域的数据可能相对较少，无法充分训练通用的大模型。领域特定的模型可以在通用模型的基础上进行微调和定制，以适应特定领域的需求
## 5 如何让大模型处理更长的文本
 1. 分块处理:长文本分割成较短的片段，然后逐个片段输入
 2. 层次建模:将文本分为段落、句子或子句等层次，然后逐层输入模型进行处理
 3. 部分生成：若只需要模型生成文本的一部分，而不是整个文本，可以只输入部分文本作为上下文，然后让模型生成所需的部分
 4. 模型结构优化：层数或参数量
# 三 LLM评测面
## 1 怎么评测
1. 语法和流畅度：人工评估+自动评估指标如困惑度（perplexity）
2. 语义准确性：准确传达了所需的含义，避免歧义，模棱两可。人工评估+领域专家
3. 上下文一致性：上下文逻辑和连贯性。人工评估
4. 信息准确性：包含的信息是否准确和可靠。人工评估+与已知信息进行对比
5. 创造性和多样性：人工评估
## 2 honest原则是如何实现
"honest"原则：生成文本时应该保持诚实和真实，不应该编造虚假信息/误导用户。
实现：
1. 数据训练：使用真实和可靠的数据进行模型的训练
2. 过滤和审查训练数据
3. 对生成结果进行监督和调整。人工审核+用户反馈+自动监测
4. 透明度和解释性：提供模型生成文本的解释和可追溯性，使用户能够了解模型生成文本的依据和过程。展示模型的输入数据+模型的结构和参数
5. 遵循道德和法律准则
## 3 模型如何判断回答的知识是训练过的已知的知识，怎么训练这种能力
实现：
1. 训练数据：使用包含已知知识的真实数据。这来自于可靠的来源
2. 监督学习：使用人工标注的数据来进行监督学习，将已知知识标注为正确答案。
3. 开放域知识库：如维基百科
4. 过滤和筛选：过滤和筛选来排除不准确或不可靠的信息
# 四 LLM软硬件配置面
建议的软件环境是什么
Python 深度学习框架 GPU支持 NumPy、Pandas  文本处理库（NLTK）存储和缓存（Mysql Hadoop HDFS）可视化工具(TensorBoard、Matplotlib) Jupyter、PyCharm
分布式计算、云计算平台

# 五 LLM推理面
## 1 大模型推理时显存涨的那么多还一直占着？
1. 模型参数
2. 输入数据
3. 中间计算结果：在推理过程中，模型会进行一系列的计算操作，生成中间结果占用显存。
4. 内存管理策略：某些深度学习框架在推理时采用了一种延迟释放显存的策略，即显存不会立即释放，而是保留一段时间以备后续使用
## 2 大模型在gpu和cpu上推理速度如何
1. GPU推理速度快：GPU具有大量的并行计算单元，可以同时处理多个计算任务。
2. CPU推理速度相对较慢：相较于GPU，CPU的计算能力较弱，主要用于通用计算任务。
3. 使用GPU加速推理：深度学习框架提供GPU加速功能，如CUDA或OpenCL。加速库可以将计算任务分配给GPU并利用其并行计算能力，从而加快大语言模型的推理速度。
## 3 推理速度上，int8和fp16比起来怎么样
INT8（8位整数量化）和FP16（半精度浮点数）相对于FP32（单精度浮点数）可以带来加速。因为INT8和FP16的数据类型所需的【内存和计算资源较少】，从而可以加快推理速度
INT8在相同的内存空间下可以【存储更多的数据】，从而可以在相同的计算资源下进行【更多的并行计算】。这可以提高【每秒推理操作数】（Operations Per Second，OPS）
## 4 大模型有推理能力吗？
有
推理指训练后，使用已经训练好的模型对新的输入数据进行预测、生成或分类等任务。大语言模型可以通过输入一段文本或问题，然后生成相应的回答或补全文本

## 5 省内存的大语言模型训练/微调/推理方法
1. 通过共享模型中的参数：可以在不同的位置共享相同的嵌入层或注意力机制。
2. 梯度累积（Gradient Accumulation）：训练过程中，将多个小批次的梯度累积起来，然后进行一次参数更新。
3. 梯度裁剪（Gradient Clipping）：限制梯度的大小，避免梯度爆炸
4. 分布式训练（Distributed Training）：训练过程分布到多台机器
5. 量化（Quantization）：高精度表示转换为低精度表示（INT8或FP16）
6. 剪枝（Pruning）：去除冗余或不重要的模型参数
7. 蒸馏（Knowledge Distillation）：使用较小的模型
8. 分块处理（Chunking）：输入数据或模型分成较小的块进行处理

# 六 LLM微调面
## 1 需要多少显存
全参数微调所需的显存量取决于
模型的大小（参数量）    批量大小   训练数据的维度     训练设备的显存限制
## 2 为什么SFT之后感觉LLM傻了
进行SFT之后，可能观察到基座模型性能下降，原因：
1. 数据偏移：微调数据集与基座模型预训练的数据分布不同。
2. 非典型标注：微调数据集的标注存在错误或不准确的标签
3. 过拟合：微调数据集相对较小/模型的容量（参数数量）较大，模型会过拟合微调数据，导致在新的输入上表现不佳
4. 缺乏多样性：微调数据集可能缺乏多样性，未能涵盖新任务可能遇到的各种case
对策：
    增加数据的多样性和覆盖范围。
    - 仔细检查微调数据集的标注，确保标签的准确性和一致性。
    - 正则化技术（如权重衰减、dropout）减少过拟合。
    - 进行数据增强，变换或扩充来增加多样性。
    - 使用更复杂的模型架构或调整模型的超参数，以提高模型的性能和泛化能力。
## 3 SFT的微调步骤
1. 收集原始数据
2. 标注数据
3. 划分数据集：训练集、验证集和测试集。
4. 数据预处理：文本清洗、分词、去除停用词、词干化等处理步骤。
5. 格式转换：txt json excel csv
6. 模型微调
7. 模型评估：测试集评估
## 4 领域模型Continue PreTrain 数据选取
1. 领域相关数据
2. 领域专家标注
3. 伪标签：自动化的方法生成伪标签。
4. 数据平衡：如果某个类的数据样本较少，数据增强技术/对该类别过采样，以平衡各个类别的数据量。
5. 数据质量控制：筛选和过滤数据。
6. 数据预处理：分词、去除停用词、标准化
## 5 领域数据训练后，通用能力往往会有所下降，如何缓解模型遗忘通用能力
1. 保留通用数据：保留一部分通用数据训练。
2. 增量学习：增量学习（Incremental Learning），将领域数据与通用数据交替进行训练。
3. 预训练和微调：在领域数据训练之前，使用大规模通用数据进行预训练，获得通用基础模型。然后，在领域数据上进行微调，以适应特定领域的任务。
4. 强化学习：给模型设置奖励机制，鼓励模型在领域任务上表现好，并保持一定的通用能力。
5. 领域适应技术：领域自适应（Domain Adaptation）和领域对抗训练（Domain Adversarial Training），帮助模型在不同领域之间进行迁移学习，从而减少遗忘通用能力的问题。
6. 数据重采样：使模型在训练过程中能够更多地接触到通用数据
## 6 领域模型Continue PreTrain ，如何让模型在预训练过程中就学习到更多的知识
1. 多任务学习：引入多个任务  领域相关的任务+通用的语言理解任务
2. 多领域数据：使用不同领域的数据
3. 大规模数据：更大规模的数据
4. 数据增强：随机遮挡、词替换、句子重组
5. 自监督学习：设计自动生成的标签或任务，让模型在无监督的情况下进行预训练。例如，可以设计一个掩码语言模型任务，让模型预测被掩码的词语。这样可以使模型在预训练过程中学习到更多的语言知识。
## 7 SFT操作的时候，基座模型选用Chat还是Base
任务是对话生成相关的，可以选择ChatGPT模型作为基座模型；如果任务是单轮文本生成或非对话生成，可以选择Base GPT模型作为基座模型
## 8 领域模型词表扩增是不是有必要的
有助于提升模型在特定领域任务上的性能
考虑：
1. 领域特定词汇
2. 领域特定上下文：领域任务中，词汇的含义可能会受到特定上下文的影响。领域任务中的上下文与通用预训练模型的训练数据中的上下文有较大差异，那么词表扩增可以帮助模型更好地理解和处理领域特定的上下文。
3. 数据稀缺性：目标领域的训练数据相对较少，而通用预训练模型的词表较大->词表扩增可以帮助模型更好地利用预训练模型的知识，并提升在目标领域任务上的性能。
## 9 训练中文大模型有啥经验
1. 数据预处理：中文分词、去除停用词、词性标注、拼音转换等。
2. 数据增强：同义词替换、随机插入或删除词语、句子重组
3. 字词级别的表示
4. 预训练模型：使用中文语料上预训练的模型作为初始模型
5. 中文特定的任务：中文特定的工具或者模型来辅助训练。THULAC、LTP等中文NLP工具包。
6. 计算资源：使用云计算平台或者分布式训练来加速训练过程。
7. 超参数调优：网格搜索、随机搜索或者基于优化算法的自动调参方法来寻找最佳的超参数组合。
## 10 指令微调好处
	指令调整模型接收一对输入和输出，描述引导模型的任务。eg：
	Instruction：写一个周末有趣的活动清单
	Output：t徒步旅行，去公园度过一天，野餐，看电影晚上
	我们要求模型学习对整个文本字符串进行语言建模，例如“Instruction：写一个周末有趣的事情清单 Output：在公园度过一天、野餐、看电影”。
好处：
1. 个性化适应：模型更好地适应目标任务特点。
2. 提升性能：
3. 控制模型行为：引入特定的指令或约束，以约束模型的行为，更符合特定任务的需求。
4. 数据效率：特定数据可能相对稀缺或难以获取。指令微调利用大模型在通用数据上的预训练知识，结合少量特定任务数据进行微调，在数据有限的情况下获得更好的性能。
5. 提高训练效率：指令微调可在已经训练好的大模型的基础上进行微调，避免从头开始训练的时间和资源消耗
## 11  预训练和微调哪个阶段注入知识的
在预训练注入知识，
注：预训练模型通过大规模通用数据的训练，学习到了丰富的语言知识和表示能力，为后续的微调阶段提供了基础。微调阶段则是在预训练模型的基础上，使用特定任务的数据进行进一步训练和调整，以提升性能
## 12 让模型学习某个领域或行业的知识，是应该预训练还是应该微调
预训练在大规模通用数据进行，提供通用语言理解表示能力。预训练模型具有泛化能力；
微调需要较少的任务数据，预训练模型已经具备语言理解/泛化能力。微调，在预训练模型基础，利用特定领域的数据进行有针对性调整，更好地适应目标领域的需求
##  13 微调后的模型出现能力劣化，灾难性遗忘是怎么回事
灾难性遗忘（Catastrophic Forgetting）：在模型微调过程中，当模型在新任务上进行训练时，可能会忘记之前学习到的知识，导致在旧任务上的性能下降
原因：
1. 数据分布差异：微调新任务数据vs预训练数据分布存在大差异。模型过度调整以适应新任务，旧任务上的性能下降。
2. 参数更新冲突：微调时，模型参数被更新->旧知识覆盖丢失。新任务梯度更vs与旧任务的梯度更新冲突->旧任务知识遗忘。
对策：
1. 重播缓冲区（Replay Buffer）：微调过程中使用缓冲区存储旧任务样本，旧任务的样本+新任务的样本混合训练->保留旧任务的知识
2. 弹性权重共享（Elastic Weight Consolidation）：引入正则化项，限制模型参数的变动范围，保护之前学习到的知识
3. 增量学习（Incremental Learning）：微调过程分为多个阶段，每个阶段只微调一小部分参数。逐步引入新任务，减少参数更新冲突
4. 多任务学习（Multi-Task Learning）：微调过程中，同时训练多个相关任务，提高模型的泛化能力和抗遗忘能力
## 14 LLM进行SFT操作的时候在学习什么
任务特定的标签预测、上下文理解和语言模式、特征提取和表示学习、任务相关的优化
## 15 预训练和SFT操作有什么不同
1. 目标：
	预训练：通过无监督学习从大规模的文本语料库中学习语言模型的表示能力和语言知识。自我预测任务，例如掩码语言模型（Masked Language Model，MLM）或下一句预测（Next Sentence Prediction，NSP）等
	有监督微调：在特定的任务上进行训练，利用预训练阶段学到的语言表示和知识，适应特定任务的要求。
2. 数据：
	预训练：大规模的无标签文本数据，模型自我预测任务来学习语言模型。
	有监督微调：有标签的任务相关数据
3. 训练方式：
	预训练：无监督的方式进行训练，最大化预训练任务的目标函数来学习语言模型的表示能力。
	有监督微调：有监督的方式，最小化损失函数来学习任务相关的特征和模式。使用预训练模型的参数作为初始参数
## 16 样本量规模增大，训练出现OOM
减小批量大小、分布式训练、内存优化技术、减少模型规模、增加硬件资源、优化数据处理
## 17 SFT 如何对样本进行优化
数据清洗和预处理、数据增强、标签平衡、样本选择、样本权重、样本组合和分割、样本筛选和策略

