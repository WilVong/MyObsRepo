# 一 LLM基础面
## 1.主流开源体系
1. GPT（Generative Pre-trained Transformer）系列：OpenAI的 Transformer架构 
2. BERT（Bidirectional Encoder Representations from Transformers）：谷歌的 在无标签文本上进行预训练，然后在特定任务上进行微调
3. XLNet：CMU和Google Brain的  Transformer架构  自回归预训练 
4. RoBERTa：Facebook的  Transformer架构  BERT的改进，更大规模数据和更长训练时间
5. T5（Text-to-Text Transfer Transformer）：Google的  Transformer架构  多任务预训练模型
## 2.语言模型结构分类
![[Pasted image 20231101010848.png]]
1. Encoder-Decoder（Transformer）: Encoder 部分是 Masked Multi-Head Self-Attention，Decoder 部分是 Casual Multi-Head Cross-Attention 和 Casual Multi-Head Self-Attention 兼具。
2. 因果语言模型（Causal Language Model, CLM）: 即Transformer的Decoder，比如GPT。也叫自回归语言模型（Auto-Regressive Language Models）
3. 掩蔽语言模型（Masked Language Model, MLM）: 即Transformer的Encoder，学习过程，能看到待预测词的前后内容，如BERT、ERNIE、ALBERT、RoBERTa
4. 前缀语言模型（Prefix language model）：如UniLM。前缀语言模型在抽取输入文本特征时用了 Fully-Visible Mask（Encoder 用的掩码，能看到「过去」和「未来」）；生成本文部分则与自回归语言模型一样，只看到左侧
## 3.涌现能力的原因
1. 数据量的增加
2. 计算能力的提升
3. 模型架构的改进
4. 预训练和微调的方法：无标签数据上预训练，特定任务上进行微调
## 4.常见大模型LLM的架构
 Transformer架构  自注意力机制  多头注意力 前馈神经网络  预训练和微调
# 二 LLM进阶面

 
