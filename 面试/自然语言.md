# 1.MacBert
MacBERT与BERT共享相同的预训练任务，但有一些修改。 对于MLM任务，我们执行以下修改。

- 我们使用全词masked以及Ngram masked策略来选择候选token来masked，单词级别的unigram到4-gram的比例为40％，30％，20％，10％。

- 我们提议不要使用[MASK]token进行mask，因为在token微调阶段从未出现过[MASK]，我们提议使用类似的单词进行masking。 通过使用基于word2vec(Mikolov et al。，2013)相似度计算的同义词工具包(Wang and Hu，2017)获得相似的单词。 如果选择一个N-gram进行masked，我们将分别找到相似的单词。 在极少数情况下，当没有相似的单词时，我们会降级以使用随机单词替换。

- 我们对15％比例的输入单词进行masking，其中80％替换为相似的单词，10％将替换为随机单词，其余10％则保留原始单词。

# 2.RoBERTa
- RoBERTa基本没有模型创新，主要是在BERT基础上做了几点调整： 1）训练时间更长，batch size更大，训练数据更多； 2）移除了next predict loss； 3）训练序列更长； 4）动态调整Masking机制。 5) Byte level  BPE  RoBERTa is trained with dynamic masking
- static masking: 原本的BERT采用的是static mask的方式，就是在`create pretraining data`中，先对数据进行提前的mask，为了充分利用数据，定义了`dupe_factor`，这样可以将训练数据复制`dupe_factor`份，然后同一条数据可以有不同的mask。注意这些数据不是全部都喂给同一个epoch，是不同的epoch，例如`dupe_factor=10`， `epoch=40`， 则每种mask的方式在训练中会被使用4次。
- dynamic masking: 每一次将训练example喂给模型的时候，才进行随机mask。
- RoBERTa使用了GPT2的 byte BPE 实现，使用的是byte（字节）而不是unicode characters作为subword的单位

# 3.wwm（全词mask）
**Whole Word Masking (wwm)**是谷歌在2019年5月31日发布的一项BERT的升级版本，主要更改了原预训练阶段的训练样本生成策略。
![[Pasted image 20230908010044.png]]
- MLM时，mask字->mask分词 (对组成同一个词的汉字全部mask，即全词mask)
- 目的：预训练时，模型能学习到词的语义信息，训练完成后字的embedding具有词的语义信息，对中文NLP任务是友好的。

Bert参数量：embedding + MHA + 层归一化 + 前向网络
https://zhuanlan.zhihu.com/p/357353536
# 4.MRC
## 4.1定义
即给定上下文 C 和问题 Q，机器阅读理解任务要求模型通过学习函数 F 使得 A = F (C, Q) 给出问题 Q 的正确答案 A。
### 完形填空；
![[Pasted image 20230908015100.png]]
### 多项选择；
![[Pasted image 20230908015115.png]]
### 跨度提取/片段提取；
![[Pasted image 20230908015130.png]]
### 自由问答
![[Pasted image 20230908015141.png]]

针对片段提取，提出了边界模型，也就是只预测答案的开始和结束位置；
MRC模型是指通过query来抽取出context中对应的answer spans，本质上来说属于2个多分类任务，即需要对context中的每个token输出其是否为answer span的starting position和ending position。

## 4.2 NER使用MRC:（A Unified MRC Framework for Named Entity Recognition）
![[Pasted image 20230909220623.png]]
在MRC中，跨度选择有两种策略,一种是  使用两个2进制分类器，一个用来预测每个token是否为开始索引，另一个用来预测每一个token是否为结束索引。这种策略允许在给定上下文和特定查询下输出多个开始索引和结束索引，有潜力根据 qy 提取出所有的相关实体。
![[Pasted image 20230909222818.png]]
![[Pasted image 20230909231606.png]]
![[Pasted image 20230909232500.png]]
# 5 指标
1 [NLP|分类与匹配的各类评价指标 | codewithzichao](https://codewithzichao.github.io/2020/05/12/NLP-%E5%88%86%E7%B1%BB%E4%B8%8E%E5%8C%B9%E9%85%8D%E7%9A%84%E5%90%84%E7%B1%BB%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/)
2 [一文带你理解｜NLP评价指标 BLEU 和 ROUGE（无公式） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/647310970)

# 6 基于Langchain agent的企微运营
## 6.1背景
运营人员成本过高，期望使用大模型技术设计对话机器人与客户沟通，降低人工成本
### 6.1.1难点
1.在运营、销售场景下，由于对话目的不同，模型需要有运营销售的沟通话术风格：预训练
2.营销场景下，模型不具备完善的垂直领域知识，很多问题回答不准确，有幻觉：利用公司特有的各类产品，折扣，活动政策，文档等知识，构建知识库，将外挂文档加入到模型：QA库，文档库
## 6.2方案
### 6.2.1 SOP流程
![[Pasted image 20231210151746.png]]
- 通过接口获取&输出结构化信息，包括历史对话，客户信息
- 构建状态分析链：根据历史对话判断当前对话阶段，状态：描述&动作
- 将状态结果和历史对话，客户信息送入agent，agent包含工具，每种状态对应的动作为一个工具。自定义工具方法：输入:根据已有信息构建提示词作为输入字符，输出:str
- 生成回复后，使用结果链判断当前回复是否合理，构建输出解析器（OutputParser）获取关键信息，将包括回复在内的所有信息构成json，通过接口传递后端

ps:StageAnalyzerChain
环节描述+环节动作
- 1.介绍问候 
- 2.信息核实 动作：核对纠正的信息，总结全部的信息 描述：客户回答，纠正
- 3.信息询问 动作：询问一个缺失信息  描述：客户触发关键词，肯定，
- 4.处理异议 动作：使用Agent tool:RetrievalQA链+描述
- 5.结束对话 动作：话术收尾 
## 6.3算法点
### 6.3.1Agent
RAG：向量库->RetrievalQA链 
向量库构建：1.运营人员人工收集问答对(4百条) 2.运营对话样本（70w->20w）+内部部门知识（16文档）->分割文本->提示词和百川生成（20w） +去重(4w)质检 (9k条)
q-q匹配知识库构建：
0.q-d匹配中，我们的场景下d中存在脏数据和冗余，不是每句话都有有效信息；q-d匹配中，q和d一个是疑问句，一个是陈述句，匹配效果不如q-q匹配
1.切割文本：对话数据：chunk:512+重叠64 知识文本：256+重叠32
2.将切割文本送入百川，尽可能生成 问题： + 回答： 对
3.去重SimHash算法 (64位 汉明距离3)
4.人工质检

去重： https://zhuanlan.zhihu.com/p/43640234?utm_id=0

模型向量化:bge微调
样本：问题->百川生成样本增强，作为正样本 相似度>0.7     问题向量化后取总样本中相似度0.3-0.6作为负样本，尽可能大  得到7k三元组
模型： 预训练：retroMae 微调：对比学习（正例距离更近，负例距离更远）
评估：MRR@k Recall@k  (Mean Reciprocal Rank平均倒数排序) 测试集构建：百川生成的总query库（知识库4w），采样一条q，根据相似度召回20条相似正例，人工质检 得到1.2k百q-qs样本对（请求）
结果对比：微调后recall@1:0.78->0.86 recall@5:0.86->0.95 mMRR@5:0.81->0.88

### 6.3.2 自定义LLM

```
class CustomLLM(LLM):
	def _llm_type(self) -> str:#返回自定义模型的名字
	def _identifying_params(self) -> Mapping[str, Any]:#返回参数字典
	def _call(self,  prompt: str, stop: Optional[List[str]] = None, run_manager: Optional[CallbackManagerForLLMRun] = None,  **kwargs: Any,  ) -> str:#返回模型调用字符串
```
Baichuan2-7B-Chat
结构：扩词表，sentence piece RoPE SwiGLU RMSNorm xFormers2注意力 混合精度训练 max-z loss 对logit值进行归一化 对齐：RLHF
样本：多轮对话样本stf
去重：哈希算法
过滤：ppl
epoch=3 maxlenth=1024 gpu=2
部署：8 bit量化-8G  fastapi
https://www.megengine.org.cn/doc/master/zh/getting-started/deploy/fast-api.html
