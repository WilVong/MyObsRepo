# 1.MacBert
MacBERT与BERT共享相同的预训练任务，但有一些修改。 对于MLM任务，我们执行以下修改。

- 我们使用全词masked以及Ngram masked策略来选择候选token来masked，单词级别的unigram到4-gram的比例为40％，30％，20％，10％。

- 我们提议不要使用[MASK]token进行mask，因为在token微调阶段从未出现过[MASK]，我们提议使用类似的单词进行masking。 通过使用基于word2vec(Mikolov et al。，2013)相似度计算的同义词工具包(Wang and Hu，2017)获得相似的单词。 如果选择一个N-gram进行masked，我们将分别找到相似的单词。 在极少数情况下，当没有相似的单词时，我们会降级以使用随机单词替换。

- 我们对15％比例的输入单词进行masking，其中80％替换为相似的单词，10％将替换为随机单词，其余10％则保留原始单词。

# 2.RoBERTa
- RoBERTa基本没有模型创新，主要是在BERT基础上做了几点调整： 1）训练时间更长，batch size更大，训练数据更多； 2）移除了next predict loss； 3）训练序列更长； 4）动态调整Masking机制。 5) Byte level BPE RoBERTa is trained with dynamic masking
- static masking: 原本的BERT采用的是static mask的方式，就是在`create pretraining data`中，先对数据进行提前的mask，为了充分利用数据，定义了`dupe_factor`，这样可以将训练数据复制`dupe_factor`份，然后同一条数据可以有不同的mask。注意这些数据不是全部都喂给同一个epoch，是不同的epoch，例如`dupe_factor=10`， `epoch=40`， 则每种mask的方式在训练中会被使用4次。
- dynamic masking: 每一次将训练example喂给模型的时候，才进行随机mask。
- RoBERTa使用了GPT2的 byte BPE 实现，使用的是byte（字节）而不是unicode characters作为subword的单位

# 3.wwm（全词mask）
**Whole Word Masking (wwm)**是谷歌在2019年5月31日发布的一项BERT的升级版本，主要更改了原预训练阶段的训练样本生成策略。
![[Pasted image 20230908010044.png]]
- MLM时，mask字->mask分词 (对组成同一个词的汉字全部mask，即全词mask)
- 目的：预训练时，模型能学习到词的语义信息，训练完成后字的embedding具有词的语义信息，对中文NLP任务是友好的。

# 4.MRC
## 4.1定义
即给定上下文 C 和问题 Q，机器阅读理解任务要求模型通过学习函数 F 使得 A = F (C, Q) 给出问题 Q 的正确答案 A。
### 完形填空；
![[Pasted image 20230908015100.png]]
### 多项选择；
![[Pasted image 20230908015115.png]]
### 跨度提取/片段提取；
![[Pasted image 20230908015130.png]]
### 自由问答
![[Pasted image 20230908015141.png]]

针对片段提取，提出了边界模型，也就是只预测答案的开始和结束位置；
MRC模型是指通过query来抽取出context中对应的answer spans，本质上来说属于2个多分类任务，即需要对context中的每个token输出其是否为answer span的starting position和ending position。

## 4.2 NER使用MRC:（A Unified MRC Framework for Named Entity Recognition）
![[Pasted image 20230909220623.png]]
在MRC中，跨度选择有两种策略,一种是  使用两个2进制分类器，一个用来预测每个token是否为开始索引，另一个用来预测每一个token是否为结束索引。这种策略允许在给定上下文和特定查询下输出多个开始索引和结束索引，有潜力根据 qy 提取出所有的相关实体。
![[Pasted image 20230909222818.png]]
![[Pasted image 20230909231606.png]]
![[Pasted image 20230909232500.png]]
# 5 指标
1 [NLP|分类与匹配的各类评价指标 | codewithzichao](https://codewithzichao.github.io/2020/05/12/NLP-%E5%88%86%E7%B1%BB%E4%B8%8E%E5%8C%B9%E9%85%8D%E7%9A%84%E5%90%84%E7%B1%BB%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/)
2 [一文带你理解｜NLP评价指标 BLEU 和 ROUGE（无公式） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/647310970)

