# 1.MacBert
MacBERT与BERT共享相同的预训练任务，但有一些修改。 对于MLM任务，我们执行以下修改。

- 我们使用全词masked以及Ngram masked策略来选择候选token来masked，单词级别的unigram到4-gram的比例为40％，30％，20％，10％。

- 我们提议不要使用[MASK]token进行mask，因为在token微调阶段从未出现过[MASK]，我们提议使用类似的单词进行masking。 通过使用基于word2vec(Mikolov et al。，2013)相似度计算的同义词工具包(Wang and Hu，2017)获得相似的单词。 如果选择一个N-gram进行masked，我们将分别找到相似的单词。 在极少数情况下，当没有相似的单词时，我们会降级以使用随机单词替换。

- 我们对15％比例的输入单词进行masking，其中80％替换为相似的单词，10％将替换为随机单词，其余10％则保留原始单词。

# 2.RoBERTa
- RoBERTa基本没有模型创新，主要是在BERT基础上做了几点调整： 1）训练时间更长，batch size更大，训练数据更多； 2）移除了next predict loss； 3）训练序列更长； 4）动态调整Masking机制。 5) Byte level BPE RoBERTa is trained with dynamic masking
- static masking: 原本的BERT采用的是static mask的方式，就是在`create pretraining data`中，先对数据进行提前的mask，为了充分利用数据，定义了`dupe_factor`，这样可以将训练数据复制`dupe_factor`份，然后同一条数据可以有不同的mask。注意这些数据不是全部都喂给同一个epoch，是不同的epoch，例如`dupe_factor=10`， `epoch=40`， 则每种mask的方式在训练中会被使用4次。
- dynamic masking: 每一次将训练example喂给模型的时候，才进行随机mask。
- RoBERTa使用了GPT2的 byte BPE 实现，使用的是byte（字节）而不是unicode characters作为subword的单位

# 3.wwm（全词mask）
**Whole Word Masking (wwm)**是谷歌在2019年5月31日发布的一项BERT的升级版本，主要更改了原预训练阶段的训练样本生成策略。
![[Pasted image 20230908010044.png]]
- MLM时，mask字->mask分词 (对组成同一个词的汉字全部mask，即全词mask)
- 目的：预训练时，模型能学习到词的语义信息，训练完成后字的embedding具有词的语义信息，对中文NLP任务是友好的。

# 4.MRC
## 4.1定义
即给定上下文 C 和问题 Q，机器阅读理解任务要求模型通过学习函数 F 使得 A = F (C, Q) 给出问题 Q 的正确答案 A。
### 完形填空；
![[Pasted image 20230908015100.png]]
### 多项选择；
![[Pasted image 20230908015115.png]]
### 跨度提取/片段提取；
![[Pasted image 20230908015130.png]]
### 自由问答
![[Pasted image 20230908015141.png]]

针对片段提取，提出了边界模型，也就是只预测答案的开始和结束位置；
MRC模型是指通过query来抽取出context中对应的answer spans，本质上来说属于2个多分类任务，即需要对context中的每个token输出其是否为answer span的starting position和ending position。

## 4.2 NER使用MRC:（A Unified MRC Framework for Named Entity Recognition）
![[Pasted image 20230909220623.png]]
在MRC中，跨度选择有两种策略,一种是  使用两个2进制分类器，一个用来预测每个token是否为开始索引，另一个用来预测每一个token是否为结束索引。这种策略允许在给定上下文和特定查询下输出多个开始索引和结束索引，有潜力根据 qy 提取出所有的相关实体。
![[Pasted image 20230909222818.png]]
![[Pasted image 20230909231606.png]]
![[Pasted image 20230909232500.png]]
# 5 指标
1 [NLP|分类与匹配的各类评价指标 | codewithzichao](https://codewithzichao.github.io/2020/05/12/NLP-%E5%88%86%E7%B1%BB%E4%B8%8E%E5%8C%B9%E9%85%8D%E7%9A%84%E5%90%84%E7%B1%BB%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/)
2 [一文带你理解｜NLP评价指标 BLEU 和 ROUGE（无公式） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/647310970)

# 6 基于Langchain agent的企微运营
## 6.1背景
与客户沟通，设计对话机器人，替代人工，降低人力成本
### 6.1.1难点
特定的场景（运营、销售）下，由于对话目的不同，模型需要有特定的沟通话术（风格）：预训练
营销场景下，首先模型（LLM）本身不具备完善的垂域知识，利于公司特有的产品库、优惠活动政策等，这就需要构建知识库（文档库），并将正确的知识加入到模型：QA库，文档库 | 快递/快运知识库，产品知识库，折扣知识库
通过构建知识库（QA库、文档库），利用文本召回算法，将正确的知识融入到机器人对话的过程中，这部分也被称为RAG（具有逻辑思考、工具调用的能力；RAG（Retrieval Augment Generation），也就是召回增强地生成
## 6.2框架
### 6.2.1 流程图
![[Pasted image 20231210151746.png]]
![](https://apijoyspace.jd.com/v1/files/PJlmztovblE0xXmjjT4W/link)
- 依据SOP流程模拟运营人员跟客户获取、确认、更正信息，输出结构化客户基本信息（可以用于SDR语音抽取，以减少SDR填写，需要衡量准确率），提供对应的接口，以收集反馈的问题；几个非标准流程的运营人员沟通的例子
- 识别不同客户状态发送对应消息
## 6.2.2 阶段
阶段1: 运营人员与客户沟通，目的是：确认客户需求意愿；确认客户信息是否正确；获得客户信息；
阶段2: 运营人员通知销售
阶段3: 运营人员拉群，销售人员继续和客户沟通，开始报价阶段

## 6.3各模块
### 6.3.1 Memory

输出解析器：
![[Pasted image 20231210221943.png]]
![[Pasted image 20231210222139.png]]
![[Pasted image 20231210222243.png]]
![[Pasted image 20231210222348.png]]
### 6.3.2Agent
RetrievalQA链
### 6.3.3Stage 
StageAnalyzerChain -> ConversationChain
环节描述+环节动作
- 1.介绍问候 
- 2.信息核实 动作：核对纠正的信息，总结全部的信息 背景：客户回答，纠正
- 3.信息询问 动作：询问一个缺失信息  背景：客户触发关键词，肯定，
- 4.处理异议 动作：使用Agent tool:RetrievalQA链
- 5.结束对话 动作：话术收尾 + 返回格式化关键信息

