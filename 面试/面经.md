# 一 大模型多家
作者：大模型技术社区  
链接：https://zhuanlan.zhihu.com/p/662549672  
1.训练大模型的内容，比如模型参数大小，训练一些细节

2.大模型训练的细节之类的，模型端，框架端

3.transformer，训练，分布式，如何处理训练一些问题比如loss spike

4.框架为主的，一面问了很多框架内容，各种模型切分方式

5.flash-attention

6.数据组：大模型数据处理

7.框架端的内容，分布式训练的切割和device之间交流

8.不同device之间communication怎么做，底层communication算法有啥，我答案里有ring-reduce，然后再深入问了reduce底层如何实现

9.做RLHF，强化学习，微调的

10.多头注意力，coding轮，概念轮都考。复习的点包括：时间/空间复杂度，优化（kv-cache，MQA，GQA），手写多头代码。各种Norm，这个频率也不低，不过比较标准的内容，没有啥特意要说的，有的考手写，有的考概念和理解（为什么管用）【S】

11.框架相关内容，各种并行方式，优缺点。DeepSpeed，Megatron可以看看源代码，Flash-Attention等内容。这个点也经常考代码题【S】

12.BERT，GPT等比较主流大模型，一些细节，比如位置编码，训练loss，激活，架构些许不同这种。自回归重点。【S】

13大模型训练，这个可能主要是工作经验相关，经常问比如训练loss炸掉了，如何解决，一些技巧之类的。面试时有些面试官会问一些很细节的东西，感觉是在确认确实上手跑过基座训练不是吹水。【A】

14.数据预处理，BPE，tokenization，mask相关概念和对模型/训练影响，数据配比（有paper）。【A】

15.evaluation，如何评估大模型，安全性，有效性，公开数据，个别考过手写eval框架（多选，生成）。【B】

# 二 强化学习相关
作者：一蓑烟雨  
链接：https://zhuanlan.zhihu.com/p/659551066  

- PPO算法中使用GAE的好处以及参数 γ\gamma 和 λ\lambda 的作用是什么
- PPO算法和DQN算法的区别是什么
- 有哪些PPO算法的调参经验
- 在线强化学习和离线强化学习在技术和应用场景上有什么区别
- 强化学习和大模型之间的关联是什么
- 如何评估大模型中数据集的质量
- 目前国内一般选择基于哪些基座模型继续训练
- 国内做大模型的主要工作是哪几个部分
- 除了数据之外，还有哪些方向的工作可以进一步优化大模型的效果
- 大语言模型是怎么输出的，观察过输出的概率值吗
- 关于微调的方法有哪些
- 如果让你训练一个模型，基座，数据，finetune的方法怎么选
- 怎么解决大语言模型的幻觉问题，RLHF可以吗
- 是否看好国内做基座模型工作的前景，为什么
- 为什么模型越大，貌似更多地具备AGI的能力？这背后的逻辑是什么
- 介绍下对transformer的了解，网络结构相比于lstm有什么不同
- transformer里用到的正则化方法有哪些
- chatgpt训练过程中，奖励模型有更新吗
- chatgpt强化学习训练阶段还有什么改进的空间和思路吗
- 直接用训练reward model的数据精调模型，而不用强化学习，是否可行？为什么
- 了解bert和gpt网络结构的细节及其差异吗
- 假如reward model不太准，怎么办
- 有做过大模型训练的实践吗，有哪些收获或者感悟

